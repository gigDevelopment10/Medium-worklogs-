# Medium Project Ideas 

- [ ]**BackGrad**: Implement a basic feedforward neural network and manually code backpropagation to learn how gradients flow through the network.
- [ ] **Simple Backpropagation**: Implement a basic feedforward neural network and manually code backpropagation to learn how gradients flow through the network.
- [ ] **Autodiff Engine from Scratch**: Build an automatic differentiation (autodiff) engine like Micrograd that tracks computation graphs and calculates gradients for optimization.
- [ ] **Manual SGD Optimizer**: Implement Stochastic Gradient Descent (SGD) from scratch without relying on deep learning libraries to understand how weights are updated during training.
- [ ] **Tiny Neural Network Library**: Create a very small neural network library that includes essential layers like Dense, ReLU, and Sigmoid from scratch, similar to a mini TensorFlow or PyTorch.
- [ ] **Gradient Descent Visualization**: Build and visualize gradient descent in 2D by manually coding the loss landscape and showing how parameters are updated through iterations.
- [ ] **Convolution Layer from Scratch**: Implement a basic convolutional layer manually, without using any deep learning framework, to better understand how convolution works in CNNs.
- [ ] **Vanilla RNN Implementatio**n: Build a recurrent neural network from scratch, focusing on how the network handles sequential data and how backpropagation through time (BPTT) works.
- [ ] **Build Your Own MLP**: Construct a Multilayer Perceptron (MLP) entirely from scratch, including manual matrix multiplications, activation functions, and backpropagation.
- [ ] **Numerical vs. Analytical Gradients**: Implement both numerical and analytical gradient computations to compare their accuracy and understand potential pitfalls in gradient calculation.
- [ ] **Gradient-Free Optimization**: Build a simple network and explore gradient-free optimization methods like genetic algorithms or evolutionary strategies, manually coding the update mechanism.
